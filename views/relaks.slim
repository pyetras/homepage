== Slim::Template.new('views/_layout.slim').render(Object.new, _head.merge(_relaks)) do
  markdown:
    I love the idea behind declarative programming languages: you start by specifying the result you want to achieve rather than how you want to achieve it.
    Some, such as SQL hide away the complexity by providing a fixed set of highly optimised and yet expressive high-level operations such as SELECT, JOIN etc.
    Others, such as my personal favourite, Prolog, allow achieving similar results while granting unlimited flexibility.

    The design of Prolog is simple and beautiful.
    It basically does just two things, and it does them really well: allow
    you to specify a search tree for your problem and then traverse that
    tree.

    The devil is in the details though: Prolog's logic based rules are pure and merciless.
    One overlooked condition, and the program would be stuck forever, generating and mindlessly following an infinite number of branches in the tree, while the solution might lie just in the *sibling* of one of the looping nodes.

    In my opinion the language of the future does not *traverse* the tree specified by the programmer, whether it's an Abstract Syntax Tree that is translated literally to a sequence of operations, or a tree of theorems and conclusions traversed by Prolog.
    Instead it *explores* it.

    Consider the following statement:

    ~~~ {.scala}
    val k = choose between 1 and 5
    select k where k == 2
    ~~~

    Looks simple enough, right? With a slight syntax alterations Prolog could do it.
    SQL could do it, if `choose` was a function that generates a table of numbers.
    Even in Python, a program where `choose` is replaced by `range` would look similar enough. How about

    ~~~ {.scala}
    val k = choose between 1.0 and 5.0
    select k where k == 2.0
    ~~~

    In theory it should be unsolvable by Prolog in the straightforward, left to right tree traversal - there is infinitely many numbers between 1.0 and 2.0.
    An implementation of this search based on computer's representation of floating numbers would eventually finish.
    If the function evaluating k was more expensive it's easy to see how a standard left-to-right search would take a long time.
    The same happens if we add more parameters to the function - the search space grows exponentially.
    If we knew that the function could be solved using e.g. [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization), we could write a much more efficient search procedure that would return a result within an acceptable error margin.

    How about

    ~~~ {.scala}
    val k = choose between 2 and 10
    optimize (knnExperiment k) by errorRate desc limit 10
    ~~~

    What I would like this expression to do is to run a K Nearest Neighbors algorithm ten times while trying to minimize `errorRate` by tweaking the parameter k.
    It turns out that KNN and other machine learning algorithms can be successfully [modelled as Gaussian Processes](https://github.com/JasperSnoek/spearmint) and iteratively optimised wrt. their hyperparameters.

    ## Relaks

    Above snippets are valid programs in Relaks - a Scala DSL.
    The similarities between Relaks and SQL are intentional - it uses a powerful *experiment database* metaphor to define search space.
    The columns are inputs and outputs of the optimised algorithms - hyperparameters and evaluated fitness.
    Once defined, a table can be queried using familiar SQL-like syntax - LIMIT, WHERE etc.
    Under the hood the table doesn't actually exist - it is generated row by row, using hints from the query to limit possibly very large search space.
    For example a WHERE clause would narrow down the set of hyperparameters and LIMIT would decrease the number of performed search iterations.
    Let's take a look at a comprehensive example:

    ~~~ {.scala}
    object Program extends
        DSLOptimizerInterpreter(SpearmintOptimizer) {
      val x = choose between 0.0 and 15.0
      val y = choose between -5.0 and 10.0

      val result = optimize (x, y) map { case Tup(x, y) =>
        val res = to (branin _) apply (x, y)
        res as 'result
      } by 'result
        limit 100

      store(result)
    }

    Program.dump()
    ~~~

    This program will try to find a minimum of the Branin-Hoo function within the specified search space, using up to 100 iterations of an optimisation algorithm.
    `store` and `dump` are hints to the interpreter to materialise and print out the experiment table at the end of the program.

    ## Work in progress...

    An experimental implementation of a language of the future with a Gaussian Process optimiser as a DSL for Scala is available on [Github](//github.com/pyetras/relaks).
    It consists of LINQ-like extensions for Scala with a query optimiser that can reorder SQL operators and fuse them with hyperparameter definitions.
    I have also implemented an interpreter for the language and a caching system that can factor out common operations between subsequent optimiser runs and cache the results in memory.
    This was part of my [Master's Thesis](/relaks.pdf).

